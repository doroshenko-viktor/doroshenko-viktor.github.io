<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>K-nearest neighbors</title><link rel="icon" href="/images/favicon.ico"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/b65b5b41f2379875.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b65b5b41f2379875.css" data-n-g=""/><link rel="preload" href="/_next/static/css/0d9654b911e08999.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0d9654b911e08999.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-fc97f3f1282ce3ed.js" defer=""></script><script src="/_next/static/chunks/main-f4ae3437c92c1efc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-85d7488a393e293e.js" defer=""></script><script src="/_next/static/chunks/211-ca3cd870e26e881a.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5B...noteKey%5D-bc6552c3ae1e3718.js" defer=""></script><script src="/_next/static/rEbIxoA2O-xiW5DUZwlG8/_buildManifest.js" defer=""></script><script src="/_next/static/rEbIxoA2O-xiW5DUZwlG8/_ssgManifest.js" defer=""></script><script src="/_next/static/rEbIxoA2O-xiW5DUZwlG8/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><section class="Layout_centeredSection__nmU9U"><ul class="NavigationBar_navbar__CBMoV"><li class="NavigationBar_navitem__LKB5F"><a class="NavigationBar_navitemContent__jEyWq" href="/">Home</a><span class="NavigationBar_separator__qvEVD">|</span></li><li class="NavigationBar_navitem__LKB5F"><button class="NavigationBar_navitemContent__jEyWq">Back</button><span class="NavigationBar_separator__qvEVD">|</span></li></ul><h1 class="NoteFormattedContent_noteTitle__Oi9sD">K-nearest neighbors</h1><article class="NoteFormattedContent_note__8cHeE">
<h2>Number Of Neighbors</h2>
<p>
  In the k-nearest neighbors (<code>k-NN</code>) algorithm, the parameter <code>k</code> (number of neighbors) denotes how many neighboring data
  points (or samples) the algorithm should consider when making a prediction for a new, previously unseen data point.
</p>
<h3>Classification</h3>
<p>
  For a given test point, the algorithm identifies the <code>k</code> training points that are closest to the point. The most frequent
  class/label among these <code>k</code> training points is then assigned to the test point.
  For instance, if <code>k = 3</code> and among the three closest training points to the test point, two belong to class <code>A</code> and one belongs
  to class <code>B</code>, the test point will be classified as class <code>A</code>.
</p>
<h3>Regression</h3>
<p>
  For a given test point, the algorithm identifies the <code>k</code> training points that are closest to the point.
  The average of the target values of these <code>k</code> training points is taken, and this average becomes the predicted value for the test point.
</p>
<p>Choosing <code>k</code>:</p>
<ul>
  <li>
    <p>
      A smaller <code>k</code> (e.g., 1 or 3) makes the algorithm more sensitive to noise in the data. It can capture fine detail in the data but can also
      be more prone to <code>overfitting</code>.
    </p>
  </li>
  <li>
    <p>
      A larger <code>k</code> provides more smoothing and can be more resistant to noise in the training data. However, it may also be more
      prone to <code>underfitting</code>, missing out on finer details.
    </p>
  </li>
</ul>
<p>
  Often, the choice of <code>k</code> is validated using techniques like cross-validation to find a value that results in the best predictive
  performance on unseen data.
</p>
<p>Distance Metric:</p>
<p>
  The concept of "nearest" in <code>k-NN</code> is based on a distance metric, usually <code>Euclidean distance</code>. However, other distance metrics,
  such as <code>Manhattan distance</code> or <code>Minkowski distance</code>, can also be used. The choice of distance metric can influence which points
  are considered "nearest."
</p>
<h2>Weights</h2>
<p>
  <code>weight</code> refers to giving different importance or influence to the neighbors when making a prediction. The idea behind using
  weights is to give more influence to nearer neighbors than to farther ones. This can sometimes improve the performance of the
  <code>k-NN</code> algorithm, especially in cases where the nearest neighbors are much more relevant for prediction than the more distant ones.
</p>
<p>There are two primary ways in which weights can be used in <code>k-NN</code>:</p>
<p><strong>Uniform Weights:</strong></p>
<p>
  This is the default and the simplest way. Each of the <code>k</code> neighbors has an equal vote in determining the predicted label
  (in classification) or contributes equally to the average (in regression).
  In this approach, whether a neighbor is the closest or the <code>kth</code> closest, it gets the same weight.
</p>
<p><strong>Distance-Based Weights:</strong></p>
<p>
  The weight of each neighbor is inversely proportional to its distance from the test point.
  For example, a common method is to set the weight of each neighbor as the inverse of its distance. So, if the distance of a
  neighbor from the test point is <code>d</code>, the weight assigned to this neighbor would be <code>1/d</code>.
</p>
<p>
  In this approach, closer neighbors have a more significant influence on the prediction than those farther away.
  The choice of using uniform or distance-based weights often depends on the specific problem and dataset. Distance-based
  weighting can be especially beneficial when the decision boundary is irregular and when closer points are inherently more
  relevant than those farther away. However, in some cases, uniform weighting might perform just as well or even better. It's
  typically a good idea to experiment with both and use techniques like cross-validation to determine which approach works better
  for a given dataset.
</p>
</article></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"noteKey":["data science","machine learning","k-nearst-neighbors"],"note":{"title":"K-nearest neighbors","date":"2023-08-23","content":"\n\u003ch2\u003eNumber Of Neighbors\u003c/h2\u003e\n\u003cp\u003e\n  In the k-nearest neighbors (\u003ccode\u003ek-NN\u003c/code\u003e) algorithm, the parameter \u003ccode\u003ek\u003c/code\u003e (number of neighbors) denotes how many neighboring data\n  points (or samples) the algorithm should consider when making a prediction for a new, previously unseen data point.\n\u003c/p\u003e\n\u003ch3\u003eClassification\u003c/h3\u003e\n\u003cp\u003e\n  For a given test point, the algorithm identifies the \u003ccode\u003ek\u003c/code\u003e training points that are closest to the point. The most frequent\n  class/label among these \u003ccode\u003ek\u003c/code\u003e training points is then assigned to the test point.\n  For instance, if \u003ccode\u003ek = 3\u003c/code\u003e and among the three closest training points to the test point, two belong to class \u003ccode\u003eA\u003c/code\u003e and one belongs\n  to class \u003ccode\u003eB\u003c/code\u003e, the test point will be classified as class \u003ccode\u003eA\u003c/code\u003e.\n\u003c/p\u003e\n\u003ch3\u003eRegression\u003c/h3\u003e\n\u003cp\u003e\n  For a given test point, the algorithm identifies the \u003ccode\u003ek\u003c/code\u003e training points that are closest to the point.\n  The average of the target values of these \u003ccode\u003ek\u003c/code\u003e training points is taken, and this average becomes the predicted value for the test point.\n\u003c/p\u003e\n\u003cp\u003eChoosing \u003ccode\u003ek\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e\n      A smaller \u003ccode\u003ek\u003c/code\u003e (e.g., 1 or 3) makes the algorithm more sensitive to noise in the data. It can capture fine detail in the data but can also\n      be more prone to \u003ccode\u003eoverfitting\u003c/code\u003e.\n    \u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\n      A larger \u003ccode\u003ek\u003c/code\u003e provides more smoothing and can be more resistant to noise in the training data. However, it may also be more\n      prone to \u003ccode\u003eunderfitting\u003c/code\u003e, missing out on finer details.\n    \u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Often, the choice of \u003ccode\u003ek\u003c/code\u003e is validated using techniques like cross-validation to find a value that results in the best predictive\n  performance on unseen data.\n\u003c/p\u003e\n\u003cp\u003eDistance Metric:\u003c/p\u003e\n\u003cp\u003e\n  The concept of \"nearest\" in \u003ccode\u003ek-NN\u003c/code\u003e is based on a distance metric, usually \u003ccode\u003eEuclidean distance\u003c/code\u003e. However, other distance metrics,\n  such as \u003ccode\u003eManhattan distance\u003c/code\u003e or \u003ccode\u003eMinkowski distance\u003c/code\u003e, can also be used. The choice of distance metric can influence which points\n  are considered \"nearest.\"\n\u003c/p\u003e\n\u003ch2\u003eWeights\u003c/h2\u003e\n\u003cp\u003e\n  \u003ccode\u003eweight\u003c/code\u003e refers to giving different importance or influence to the neighbors when making a prediction. The idea behind using\n  weights is to give more influence to nearer neighbors than to farther ones. This can sometimes improve the performance of the\n  \u003ccode\u003ek-NN\u003c/code\u003e algorithm, especially in cases where the nearest neighbors are much more relevant for prediction than the more distant ones.\n\u003c/p\u003e\n\u003cp\u003eThere are two primary ways in which weights can be used in \u003ccode\u003ek-NN\u003c/code\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUniform Weights:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\n  This is the default and the simplest way. Each of the \u003ccode\u003ek\u003c/code\u003e neighbors has an equal vote in determining the predicted label\n  (in classification) or contributes equally to the average (in regression).\n  In this approach, whether a neighbor is the closest or the \u003ccode\u003ekth\u003c/code\u003e closest, it gets the same weight.\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDistance-Based Weights:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\n  The weight of each neighbor is inversely proportional to its distance from the test point.\n  For example, a common method is to set the weight of each neighbor as the inverse of its distance. So, if the distance of a\n  neighbor from the test point is \u003ccode\u003ed\u003c/code\u003e, the weight assigned to this neighbor would be \u003ccode\u003e1/d\u003c/code\u003e.\n\u003c/p\u003e\n\u003cp\u003e\n  In this approach, closer neighbors have a more significant influence on the prediction than those farther away.\n  The choice of using uniform or distance-based weights often depends on the specific problem and dataset. Distance-based\n  weighting can be especially beneficial when the decision boundary is irregular and when closer points are inherently more\n  relevant than those farther away. However, in some cases, uniform weighting might perform just as well or even better. It's\n  typically a good idea to experiment with both and use techniques like cross-validation to determine which approach works better\n  for a given dataset.\n\u003c/p\u003e\n"}},"__N_SSG":true},"page":"/notes/[...noteKey]","query":{"noteKey":["data science","machine learning","k-nearst-neighbors"]},"buildId":"rEbIxoA2O-xiW5DUZwlG8","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>