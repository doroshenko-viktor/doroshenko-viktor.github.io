{"pageProps":{"noteKey":["algorithms","basic-algorithms"],"note":{"title":"Basic algorithms","date":"2025-10-04","content":"\n<h2>Advanced Algorithmic Strategies for Technical Interviews</h2>\n<p>Successfully passing technical interviews often requires mastering specific algorithmic patterns that ensure optimal time complexity. A strong foundation in these core strategies is essential for turning complex problems into efficient code.</p>\n<h3>1. The Two Pointers Algorithm</h3>\n<p>This technique is a linear scan that optimizes algorithms involving sequences (arrays, strings, linked lists) by tracking two positions simultaneously. It is most often used to avoid nested loops and optimize time complexity to <strong>$O(n)$</strong>.</p>\n<table>\n  <thead>\n    <tr>\n      <th align=\"left\">Approach</th>\n      <th align=\"left\">Mechanism and Efficiency</th>\n      <th align=\"left\">Key Insight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"left\"><strong>Opposite Direction</strong></td>\n      <td align=\"left\">Pointers start at opposite ends and move toward each other. Since each pointer makes only one pass, the total time complexity remains <strong>$O(n)$</strong>.</td>\n      <td align=\"left\">Exploits sorted data (Two Sum II) or checks symmetrical properties (Palindromes).</td>\n    </tr>\n    <tr>\n      <td align=\"left\"><strong>Different Speeds (Fast/Slow)</strong></td>\n      <td align=\"left\">The slow pointer moves 1x speed, while the fast pointer moves 2x speed. The distance between them increases linearly.</td>\n      <td align=\"left\"><strong>Cycle Detection (Floyd's Algorithm):</strong> If a cycle exists, the fast pointer is guaranteed to eventually <em>catch</em> the slow pointer within the loop.</td>\n    </tr>\n  </tbody>\n</table>\n<h3>2. Hash Tables and Sets</h3>\n<p>Hash tables (dictionaries in Python) are the primary tool for optimizing time complexity by sacrificing space. They are used when search speed is critical in unsorted data.</p>\n<ul>\n  <li><strong>Mechanism (The $O(1)$ Secret):</strong> Hash tables map a key to a specific memory location (index) using a <em>hash function</em>. This allows retrieval of a value (or a check for existence) in <strong>$O(1)$</strong> (constant time) on average, regardless of the data size.</li>\n  <li><strong>Space-Time Trade-off:</strong> By allocating $O(n)$ extra space (to store the table), you reduce the time complexity from $O(n^2)$ to <strong>$O(n)$</strong>.</li>\n  <li><strong>Key Insight:</strong> The solution for many problems (like Two Sum) is to look for the <strong>complement</strong> (<code>target - current\\_value</code>). A hash map allows you to check for this complement instantly.</li>\n</ul>\n<h3>3. Tree Traversal (DFS vs. BFS)</h3>\n<p>Two main approaches are used for traversing tree structures, each optimizing for a different kind of search:</p>\n<table>\n  <thead>\n    <tr>\n      <th align=\"left\">Method</th>\n      <th align=\"left\">Data Structure</th>\n      <th align=\"left\">Principle</th>\n      <th align=\"left\">When to Use</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"left\"><strong>DFS (Depth-First Search)</strong></td>\n      <td align=\"left\">Recursion (Implicit Stack)</td>\n      <td align=\"left\">Explores one branch completely (\"goes deep\") before backtracking. The depth is $1 + \\max(\\text{depth}<em>{\\text{left}}, \\text{depth}</em>{\\text{right}})$.</td>\n      <td align=\"left\">Finding the maximum depth, checking symmetry, preorder/inorder/postorder traversal.</td>\n    </tr>\n    <tr>\n      <td align=\"left\"><strong>BFS (Breadth-First Search)</strong></td>\n      <td align=\"left\">Queue (FIFO)</td>\n      <td align=\"left\">Explores all nodes level by level, moving outward from the root.</td>\n      <td align=\"left\">Level Order Traversal, finding the <strong>shortest path</strong> (as BFS naturally explores nodes closer to the source first).</td>\n    </tr>\n  </tbody>\n</table>\n<h3>4. The Stack for Order Enforcement</h3>\n<p>The Stack is an Abstract Data Type defined by the <strong>LIFO (Last-In, First-Out)</strong> principle, making it perfect for problems requiring nested symmetry or correct ordering.</p>\n<ul>\n  <li><strong>Core Application (Valid Parentheses):</strong> The stack enforces that the most recently opened element must be the first one closed.</li>\n  <li><strong>Strategy:</strong>\n    <ol>\n      <li><strong>Push:</strong> On an opening element (<code>(</code>, <code>{</code>, <code>[</code>), push the corresponding opening character onto the stack.</li>\n      <li><strong>Pop and Match:</strong> On a closing element, immediately pop the stack and check that the popped opening character exactly matches the current closing character (often done using a hash map for fast pair lookup).</li>\n    </ol>\n  </li>\n  <li><strong>Failure Conditions:</strong> A string is invalid if: (1) The elements do not match, (2) the stack is empty when a closing element is found, or (3) the stack is <strong>not</strong> empty after processing the entire string (meaning an opening element was never closed).</li>\n</ul>\n<h3>5. Linked List Pointer Manipulation</h3>\n<p>While linked list traversal often uses the Two Pointers pattern, specific problems require mastering the <strong>three-pointer technique</strong> to modify the list structure in-place.</p>\n<ul>\n  <li><strong>Reversing a Linked List:</strong> The solution requires meticulous tracking of three pointers in every iteration:\n    <ol>\n      <li><strong>Preserve:</strong> Save the link to the next node (<code>next\\_node = current.next</code>).</li>\n      <li><strong>Reverse:</strong> Reassign the current node's pointer (<code>current.next = previous</code>).</li>\n      <li><strong>Advance:</strong> Shift the <code>previous</code> and <code>current</code> pointers forward.</li>\n    </ol>\n  </li>\n  <li>This approach achieves an optimal <strong>$O(n)$</strong> time complexity with <strong>$O(1)$</strong> space complexity.</li>\n</ul>\n"}},"__N_SSG":true}